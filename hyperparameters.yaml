CUDA_FRACTION: 0.8
TASK: "light_dataset_v1_other_decimated" # arac or atis yonelim or tank or TS (traffic sign) or light_dataset_baseline_v1
GPUS: [0]
PRETRAINED: true # true daha iyi çalışıyor
FREEZE: true
DO_H_FLIP: true
FROZEN_LAYERS: [1]
MODEL: "effnet" # resnet18 or effnet
model_type: "B0"
IGNORE_OTHER: false # true works better.
IMB_POW: 0.5 # Change how much you want to tackle class imbalance.
ADV_AUG: false
TEST: false
DO_NORMALIZE: true
OTHER_HEAD: false

# Optimizer & LR scheme
# 128 batch size did not fit.
batch_size: 80 #1024  # per GPU
workers: 4

epochs: 50
# opt: 'sgd'
# momentum: 0.9

#  0.001 was too much apparently.
# lr: 0.001
# lr_scheduler: 'cosine'
lr: 0.0004 # Default is :0.0002
final_lr: 0.00001
scheduler_epochs: 10
# lr_scheduler: 'steplr'
# lr_step_size: 20 # update lr every 30 epochs.
lr_gamma: 0.1
# Current train hyps
# lr_step_size: 15
# lr_gamma: 0.3



# lr: 0.5
# lr_scheduler: 'cosineannealinglr'
# lr_warmup_epochs: 5
# lr_warmup_method: 'linear'
# lr_warmup_decay: 0.01

# Regularization and Augmentation
weight_decay: 0.001
# # EMA configuration
# model_ema: True
# model_ema_steps: 32
# model_ema_decay: 0.99998

# Energy fine tuning for OOD
# Regular train settings.
energy_epochs: 10
energy_initial_lr: 0.001 # cosine decay initial max lr 1e-3
energy_final_lr: 0.000001 # cosine decayfinal min lr 1e-6

energy_batch_size_ID: 64
energy_batch_size_OD: 128
# energy_batch_size_ID: 128 FAILED
# energy_batch_size_OD: 256
energy_batch_size_TEST: 200

# This paper took parameters from Outlier Exposure
energy_decay: 0.0005
energy_momentum: 0.9 # Was not shared in both papers (this and OE), but was shared in code.

# Specifics hyps.
# Found values are averge_energy_out:-4.3064958669650935 , averge_energy_in : -5.5923678170122875
energy_m_in: -11 #-10
energy_m_out: 1 #-0
# DEFAULT values:
# energy_m_in: -23 # As expected, imposing too small of an energy margin m in for in-distribution data may lead to difficulty in optimization and degradation in performance.
# energy_m_out: -5 # Paper note:Overall the method is not very sensitive to m out in the range chosen.

# Temperature 1 gave the best results. (shown in Supplementary Material.)
# energy_loss_weight
energy_lambda: 0.1
# Energy threshold for OOD is not deterministic. Here is the explanation, it should be chosen to achieve FPR95: https://github.com/wetliu/energy_ood/issues/8 https://github.com/wetliu/energy_ood/issues/6
energy_threshold:
energy_temperature: 1

# Other head training
# Regular train settings.
other_head_epochs: 50
other_head_initial_lr: 0.001 # cosine decay initial max lr 1e-1
other_head_final_lr: 0.00001 # cosine decayfinal min lr 1e-5
other_head_batch_size_ID: 64
other_head_batch_size_OD: 128
other_head_batch_size_TEST: 200
other_head_decay: 0.0005
other_head_momentum: 0.9 # Was not shared in both papers (this and OE), but was shared in code.
other_head_lambda: 0.1
